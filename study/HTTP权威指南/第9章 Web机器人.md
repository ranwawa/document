> 好吧.我也不知道书上为什么要介绍Web机器人而不是浏览器.或许因为它也算客户端,属于Web结构上的一环吧,并且不需要涉及到浏览器渲染特性这一坨

## 9.1 爬虫及爬行方式
- 什么是爬虫
  - 爬虫是一种应用程序
  - 自动访问网站,收集网站信息的程序
  
- 爬虫从哪开始访问网站
  - 有一个根集
  - 存放所有起始访问的网址
  
- 搜索引擎为什么需要用户提供根集
  - 有些网址不会和其他网址相互链接,是个孤岛,不提交就不会被爬虫看到
  
- 爬虫为什么会陷入死循环
  - 网址之间互相链接
  - 爬虫就会不停的去访问那几个网址
  
- 死循环的危害
  - 爬虫被困住了,无法收集其他网站的信息
  - 一直不停的访问固定网站,可能会把服务器搞死
  - 收集了大量一样的信息,搜索结果页信息完全一样
  
- 爬虫管理网址的技术
  - 利用树和散列表记录访问过的网址,以加速查询
    - 请求散列表是啥??
  - 有损的存在位图:不懂.......
  - 检查点:将访问过的URL列表保存到硬盘,以防死机
  - 分类:利用机器人集群进行收录

### 9.1.1 哪些情况可能导致死循环
  - 别名URL导致
  - 文件系统连接成环路
    - 子页面不停的指向父级页面,而父级页面里面又链接的有子页面
  - 动态Web空间生成
    - 根据不同的请求Url生成新的Url
- 什么是URL别名
  - URL字符串看上去不一样,但实际指向的是同一份资源
  
- URL有哪些别名
  - 没端口号和显示80端口号的
  - 有转义字符和没转义字符的
  - 有片段标记和没片段标记的
  - 有默认index.html和没有默认页面的
  - 网址和IP地址的
  
- 爬虫怎么处理别名的URL
  - 在访问之前,先用固定的标准规范URL
  - 比如把转义字符换成原始字符
  - 统一去掉80端口号等
  
### 9.1.2 如何避免死循环 
- 规范化URL
- 广度优先的爬行
- 节流: 限制爬虫一段时间内从一个站点获取的页面数量
- 限制URL的大小
- URL黑名单
- URL模式检测: 出现重复的URL片段时就跳出来
- 内容指纹: 给整个HTML主体MD5算一下
- 人工监视: 通过日志来排查

## 9.2 爬虫的HTTP
- 为什么大多数爬虫机器人发出的是HTTP/1.0的请求
  - 爬虫要爬行巨多的页面
  - 所以只希望发送最小的HTTP请求,就用1.0咯

- 爬虫最低要实现的首部
  - User-Agent: 说明是哪个爬虫
  - From: 留下爬虫作者联系方式  
  - Accept: 只接受希望接受的信息
  - Referer: 从哪来的
  - Host: 虚拟主机要用
  
- 爬虫为什么要发起条件请求
  - 和缓存的条件再验证一样,为了尽量节省流量
  
## 9.3 爬虫存在的问题
- 陷入死循环的爬虫可能搞死服务器
- 不停的访问失效URL会影响网站日志,降低服务器性能
- 访问越长的错误URL,影响服务器日志和性能
- 有可能访问到隐私页面
- 不停的访问动态网关,导致服务器性能

## 9.4 如何拒绝爬虫访问
- 什么是拒绝机器人访问协议
  - 就是robots.txt文件
### 9.4.1 robots.txt
- robots.txt的几个版本
  - 0.0
  - 1.0 几乎都在用这个
  - 2.0 几乎无人使用
  
- robots.txt的文件格式
  - 空行
  - 注释行
  - 规则行
```
User-Agent: xxx // 出现在这个里面的爬虫不能访问网址
Disallow: /private // 出现在这个目录里面的文件不能访问
Allow: /public // 允许访问的目录

User-agent: * // 所有的都可以访问
```

- robots的工作方式
  - 爬虫先访问robots.txt
  - 看看自己能否访问
  - 再看看哪些目录*前缀*能访问和不能访问
  - 访问网址前,先对比一下前缀再做决定
  
- robots的不足
  - 不能指定某一类型的子目录,只能一个一个罗列
  
- 为什么要缓存robots.txt文件
  - 有些网站可能一天会访问N次
  - 每次都要访问和识别robots文件是没必要的
### 9.4.2 Perl代码
> 这个就不做笔记了吧.运维相关的了
### 9.4.3 HTML元标签
- 什么是排斥标签
  - 由HTML作者控制页面是否能被爬虫爬取
  - 即robot-control标签
```
<meta name="robots" content="directive-list">
```

- robot-control相关指令
  - noindex: 不索引网页
  - nofollow: 不爬取外链
  - index: 可以索引
  - follow: 可以爬取外链
  - noarchive: 不缓存页面
  - all: index + follow
  - none: noindex + nofollow 

- 搜索引擎标签
  - 即TDK
  - 多了一个revisit-after: 但是这个怎么指定,没太大实用意义
  
## 9.5 爬虫的规范
> 这个东西是设计给开发爬虫程序的人用的.用于指导别人怎么开发爬虫.属于设计层面.感觉这个很厉害呀.以后如果要自己设计东西,特别是大型项目的时候,可以跑回来参考一下,毕竟这个顶尖人士设计出来的东西,用于世界范围的规范啊.只是现在没必要看了,现在要先把所有基础知识过一下,明年好换工作哦,先MARK一下 ,P251
  
## 9.6 搜索引擎
> 书上讲这么两页面,哪里能讲的明白,这个东西现在是日常使用的了,这些基础知识就不用做笔记啦  
  
  
  
  
  
